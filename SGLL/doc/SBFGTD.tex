\documentclass[a4paper,10pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage{array}
\usepackage{graphicx}
\usepackage{float}
\setlength{\parindent}{0pt}

%opening
\title{Generalized Top Down Parsing In Cubic Time}
\author{Arnold Lankamp}

\begin{document}

\maketitle

\begin{abstract}

In this article we will describe a generalized top down parsing algorithm. Since it's top down it will be easier for a human to understand. Additionally we will also demonstrate that, in terms of efficiency, our implementation performes excellent.

\end{abstract}

\section{Introduction}

If the abstract didn't catch your attention yet, here are some highlights:
\begin{itemize}
 \setlength{\itemsep}{0pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}

 \item Worst case cubic space and time bounds with respect to the length of the input.
 \item Worst case linear scaling relative to the size of the grammar.
 \item Linear performance on LL(k) and LR(k) grammars.
 \item Generating or hand crafting the parser code is trivial.
 \item Parse traces are easy to follow; using a debugger to step through the parse process can actually be informative.
 \item Low constant overhead.
 \item No performance hit for rules that are not left factored.
\end{itemize}
We call our parser, the Scannerless Binary Forest Generalized Top Down Parser (SBFGTD) (BF also means Bloody Fast, in case your wondering why I put it in the abbreviation).

\section{Recognizer}

\subsection{GSS}

As is common in generalized parsing we use a kind of Graph Structured Stack (GSS). In our case we generate a stack node with a unique identifier for every item in the grammar. So for for example; if we would have the following grammar: $S\,::=\,AB\,|\,BA,\,A\,::=\,a,\,B\,::=\,b$, we would assign them as follows: $S$(-1), $A$(0), $B$(1), $B$(2), $A$(3), $a$(4) and $b$(5). Every terminal and non-terminal on the right hand side of a production gets a unique identifier, regardless whether or not a comparable right hand side occurs again in another position in the grammar. These identifiers are needed to handle sharing in the GSS correctly. Each stack node consists of this identifier, edges back to their 'parents' and information about it's 'right neighbour' in the production, so we know where to go next.\\
Since the GSS only contains nodes that we expect to 'encounter', there are at most $O(N)$ of them; one of every type per location in the input string. Since a node only has one edge to each of it's possible parents per level and there are $O(N)$ levels, there are at most $O(N)$ edges per node. This means that the total algorithm becomes $O(N^3)$; in the worst case we follow $O(N^2)$ edges in total and after every reduction a sharing check may need to be done that completes in $O(N)$ time.

\subsection{Basic algorithm}

\begin{enumerate}
 \setlength{\itemsep}{0pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}

 \item Expand the left most node of every production that did not match anything yet on all stacks, until no stacks can be expanded anymore (i.e. they all have a terminal at the bottom).
 \item Match all nodes on the bottom of the stacks to the input. Reduce the ones that match, remove the ones that don't.
 \item Reduce the nodes on all stacks that matched something and have them queue the 'next' node in the production they are part of where applicable, until no reductions are possible anymore.
 \item goto 1.
\end{enumerate}

\subsection{Pseudocode}

{\small
\begin{verbatim}
parse(){
  toExpandList.push('startNode');
  expand();
  
  do{
    reduce();
    expand();
  }while(toReduceList.notEmpty());
  
  if(notAtEOI()) parse error;
}

expand(){
  while(toExpandList.notEmpty()){
    node = toExpandList.pop();
    
    Node[] children = getChildrenFor(node);
    for(childNode <- children){
      if(reducedStore.contains(childNode)){ // Child already has results
         if(notAddedEarlierInThisLoop){ // Sharing
           toReduceList.add(node);
         }
      }else{
        if(expandedStore.contains(childNode)){ // Sharing
          expandedStore.get(childNode).addEdge(node);
        }else{
          expandedStore.add(childNode);
          
          if(childNode.isTerminal()){
            toReduceList.push(childNode);
          }else{
            toExpandList.push(childNode);
          }
        }
      }
    }
  }
}

reduce(){
  while(toReduceList.notEmpty()){
    node = toReduceList.pop();
    reducedStore.add(node);
    
    if(node.hasEdges()){
      for(parent <- node.edges){
        if(toReduceList.notContains(parent)){ // Sharing
          toReduceList.push(parent);
        }
      }
    }else if(node.hasNext()){
      next = node.next;
      if(toExpand.contains(next)){ // Sharing
        next = toExpandList.get(next);
      }else if(expandedStore.notContains(next)){ // Sharing
        toExpandList.push(next);
      }
      next.addEdges(node.edges);
    }
  }
}
\end{verbatim}
}

\subsection{Example traces}

To illustrate how the recognizer works, here are some example traces.

\subsubsection{Strait forward}
$S\,::=\,AB$\\
$A\,::=\,a$\\
$B\,::=\,b$\\
input = $ab$

\begin{enumerate}
 \setlength{\itemsep}{0pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}
 
 \item expand S
 \item expect AB
 \item expand A
 \item expect a
 \item reduce a and follow edge to A
 \item move from A to B
 \item expand B
 \item expect b
 \item reduce b follow edge to B
 \item reduce AB and follow edge to S
 \item reduce S
 \item done.
\end{enumerate}

\subsubsection{Left recursive}
$S\,::=\,A$\\
$A\,::=\,Aa\,|\,a$\\
input = $aaa$

\begin{enumerate}
 \setlength{\itemsep}{0pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}
 
 \item expand S
 \item expect A
 \item expand A
 \item expect Aa $|$ a
 \item expand A $\Rightarrow$ A shared
 \item reduce a and follow edge to A
 \item move from A to a
 \item reduce Aa and follow edges to S and A
 \item parse for S is incomplete and is discarded
 \item move from A to a
 \item reduce Aa and follow edges to S and A
 \item parse for S is complete.
 \item move from A to a
 \item reduce Aa failed, since EOI has already been reached.
 \item done.
\end{enumerate}

TODO: Maybe convert to illustrations?

\section{Parser}

\subsection{Parse forest}

-Custom 'deflattenized' format (similar in intent as binarization).\\
-Consists of result nodes, prefixesLists and pairs of result nodes and prefixesLists.\\
-Every result node + prefixesList pair is unique.\\
-Every prefixesList represent all parses for a certain substring (start + end position).\\
-Every prefix in the prefixesList represents a different parse of the substring the prefixesList denotes (start + end position).\\
-Every result node represents a parse for a certain substring (start + end position).\\
-All prefixesLists used in combination with the same 'type' of result node are the same, regardless of the result's end position.\\
-Every pair is identified by it's prefixesList's start position, result node start position and result node end position; the prefix end position and result node start position are the same (naturally).

-The parse forest contains at most $O(N^{2})$ result node's and $O(N^{2})$ prefixesLists. Since all prefixesLists are shared regardless of the result node's end position that they are matched to, there are at most $O(N^{2})$ unique links from result nodes to prefixesLists. Every prefixesList can contain at most $N$ different prefixes, making the parse forest $O(N^{3})$ worst-case (see example further below).

-It is possible to output a flattened (unbound polynomial) version of the parse forest at the user's request.

\subsection{Example}

Parse tree example:
Grammar:
$S\,::=\,AAAA\,|\,AAA\,|\,AA$\\
$A\,::=\,a\,|\,aa$\\
input = $aaaa$

\begin{figure}[H]
\centering
\includegraphics[width=0.2\textwidth]{a_aa-forest.png}
\caption{(Partial) visual representation of the parse forest, showing all alternatives for '$S0$-$4$' (numbers indicate start and end position of the matched substring)}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{ p{15em} p{15em} }
Derivation & Forest path\\
\hline
S(A(a),A(a),A(a),A(a)) & A0-1 $\leftarrow$ A1-2 $\leftarrow$ A2-3 $\leftarrow$ A3-4\\
S(A(a),A(aa),A(a)) & A0-1 $\leftarrow$ A1-3 $\leftarrow$ A3-4\\
S(A(aa),A(a),A(a)) & A0-2 $\leftarrow$ A2-3 $\leftarrow$ A3-4\\
S(A(a),A(a),A(aa)) & A0-1 $\leftarrow$ A1-2 $\leftarrow$ A2-4\\
S(A(aa),A(aa)) & A0-2 $\leftarrow$ A2-4
\end{tabular}
\caption{'Flattened' version of the parse forest}
\end{table}

\subsection{Worst case example}

Worst-case parse tree example:

Grammar:\\
$S\,::=\,SSS\,|\,SS\,|\,a\,|\,epsilon$\\
input = $a\,*\,1\,-\,a\,*\,10$

\begin{table}[H]
\centering
\begin{tabular}{ | p{4em} | p{4em} | p{4em} | p{5em} | p{4em} | p{4em} | }
  \hline
  Input length & Number of nodes & Number of pairs / links & Number of parent to child pointers & Total & Total / $N^{3}$ \\
  \hline
  0 & 2 & 2 & 3 & 7 & - \\
  1 & 5 & 6 & 9 & 20 & 20 \\
  2 & 8 & 13 & 15 & 36 & 4.5 \\
  3 & 11 & 24 & 21 & 56 & 2.074 \\
  4 & 14 & 40 & 27 & 81 & 1.265 \\
  5 & 17 & 62 & 33 & 112 & 0.896 \\
  6 & 20 & 91 & 39 & 150 & 0.694 \\
  7 & 23 & 128 & 45 & 196 & 0.571 \\
  8 & 26 & 174 & 51 & 251 & 0.490 \\
  9 & 29 & 230 & 57 & 316 & 0.433 \\
  10 & 32 & 297 & 63 & 392 & 0.392 \\
  \hline
\end{tabular}
\caption{Worst-case parse tree data}
\end{table}

TODO: Check data again.\\
TODO: Improve explanation.

\subsection{Psuedocode}

{\small
\begin{verbatim}

Parser psuedocode:
parse(){
  toExpandList.push('startNode');
  expand();
  
  do{
    reduce();
    expand();
  }while(toReduceList.notEmpty());
  
  if(notAtEOI()) parse error;
  
  return findResultStore('startNode');
}

expand(){
  while(toExpandList.notEmpty()){
    node = toExpandList.pop();
    
    Node[] children = getChildrenFor(node);
    for(childNode <- children){
      if(reducedStore.contains(childNode)){
        findResultStore(node).
          addAlternative(childNode.prefixes, childNode.results);
        if(notAddedEarlierInThisLoop){ // Sharing
          toReduceList.add(node);
        }
      }else{
        if(expandedStore.contains(childNode)){ // Sharing
          expandedStore.get(childNode).addEdge(node);
        }else{
          expandedStore.add(childNode);
          
          if(childNode.isTerminal()){
            toReduceList.push(childNode);
          }else{
            toExpandList.push(childNode);
          }
        }
      }
    }
  }
}

reduce(){
  while(toReduceList.notEmpty()){
    node = toReduceList.pop();
    reducedStore.add(node);
    
    if(node.hasEdges()){
      for(parent <- node.edges){
        findResultStore(parent).
          addAlternative(node.prefixes, node.results);
        if(toReduceList.notContains(parent)){ // Sharing
          if(reducedStore.notContains(parent)){
            toReduceList.push(parent);
          }
        }
      }
    }else if(node.hasNext()){
      next = node.next;
      if(toExpandList.contains(next)){ // Sharing
        next = toExpandList.get(next);
      }else if(expandedStore.notContains(next)){ // Sharing
        toExpandList.push(next);
      }
      next.addEdges(node.edges);
      next.updatePrefixes(node.prefixes, node.results);
    }
  }
}

findResultStore(node){
  return resultStores.
    findOrCreate(node.startLocation, node.endLocation, node.name);
}

\end{verbatim}
}


NOTE: algorithm / psuedocode was reverse engineered from the actual implementation and has been made more generic. (I'm currently unsure whether or not I accidentially ommitted any special cases in the conversion).

\section{Optimizations}

-Make the algortihm breadth first / level synchronized; this reduces the amount of resources needed to keep track of sharing and reduces maximum memory usage in the general case.\\
-Match on entire literals instead of characters. This reduces stack activity and improves performance.\\
-Cache 'queued' / 'expected' children for every type of node per level, to ensure linear scaling for non-left-factored grammars and to enable further optimization.\\
-Don't store results on edges, but externally in a (temporary / reusable) per level store; this reduces memory usage, since edges can remain pointers. Additionally edges can be shared between different nodes, since often nodes are guaranteed to (either partially or entirely) have the same edges, since they are always 'queued' / 'expected' by the same 'parent(s)'. The number of edges can be reduced to $N * numberOfProductions$ instead of $N * (numberOfProductions^{2})$. In absence of a priority system that restricts 'parent' / 'child' relations, the number of edges can even be reduced to $N * numberOfLHS$'s instead of $N * numberOfProductions$ (with as positive side effect that the parser for any grammar becomes as fast as it's left-factored counterpart (if implemented properly)).\\
-Share results between nodes with the same 'name', representing the same substring, but with a different id. (For example in "$S\,::=\,AB\,|\,CB,\,A\,::=\,a,\,B\,::=\,b,\,C\,::=\,a$" both B's can share results). This increases sharing in the resulting tree and improves scaling.\\
-The sharing check on 'parents' when reducing can be eliminated. Since every parent of the same 'type' always has exactly the same children for the same position, it is sufficient to just follow the edges of the first child and only add the results to the 'result store' of the parent(s) for all following children. This changes the scaling of the parser with respect to the number of different 'sorts' in the grammar from quadratic to linear. (This optimization requires the previous optimization).\\
-Share 'prefixes' of productions in the GSS. (For example $S\,::=\,E\,+\,E\,|\,E\,-\,E$ both start with an E that matches the same substring(s), thus may as well be merged (into $S\,::=\,E\,(+\,E\,|\,-\,E)$)); this is done by simply giving both $E$'s the same id and allowing every node to have more then one 'next' node.

TODO: Add more explanation + examples for all of these.

\section{Filtering}

\subsection{Look-ahead}

-Prevents 'expects' from 'firing'. Currently one character, but has no limit.

\subsection{Follow restrictions}

-If the current node being reduced is followed by any of its associated follow restrictions, the reduction is not performed. Follow restrictions can be any IMatchable node (character / literal / ciliteral / start-of-line / end-of-line / at-column).

\subsection{Rejects}

-Any RHS can be marked as reject. If it 'returns a result', any alternatives for the corresponding LHS representing the same substring are then discarded.\\
-Partially parse time, partially post parse filtering. \\
TODO: Check correctness.

\subsection{Priorities and associativity}

-Are implemented as 'don't nest' relations. (For example E ::= E * E > E + E means that E + E can't be a child of E * E, similarly S ::= E + E {left} means that E + E can't be a child of the E on the left side of the production).\\
-Completely parse time.\\
NOTE: Disables certain optimizations (hurts badly) and is a pain to implement.

TODO: add examples.

\subsection{Actions}

-Every sort can have a action associated with it.\\
-Can be used for disambiguation, amoung other things.

\section{Benchmarks}

\subsection{Worst case}

$S\,::=\,SSS\,|\,SS\,|\,a$\\
input = $a\,*\,50\,-\,a\,*\,500$

\begin{table}[H]
\centering
\begin{tabular}{ | p{5em} | p{7em} | p{6em} | }
  \hline
  Input chars & Average Time & Lowest Time \\
  \hline
  50 & 4 & 0 \\
  100 & 18 & 10 \\
  150 & 60 & 60 \\
  200 & 154 & 150 \\
  250 & 322 & 320 \\
  300 & 596 & 590 \\
  350 & 996 & 990 \\
  400 & 1534 & 1530 \\
  450 & 2228 & 2220 \\
  500 & 3090 & 3080 \\
  \hline
\end{tabular}
\caption{Recognizer}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | p{5em} | p{7em} | p{6em} | }
  \hline
  Input chars & Average Time & Lowest Time \\
  \hline
  50 & 6 & 0 \\
  100 & 36 & 30 \\
  150 & 130 & 130 \\
  200 & 332 & 320 \\
  250 & 672 & 660 \\
  300 & 1206 & 1200 \\
  350 & 2018 & 2010 \\
  400 & 3184 & 3170 \\
  450 & 4776 & 4750 \\
  500 & 6880 & 6870 \\
  \hline
\end{tabular}
\caption{Parser}
\end{table}

NOTE: Recognizer optimized for speed; parser for memory and speed (meaning it can be faster).

TODO: Add graph.

\subsection{Grammar factoring}

This test tries to emulate a 'realistic' 'worst-case' grammar in terms of size and type of productions. I.e. lots of left recursive stuff, but no ambiguous input.

Grammar:\\
$S\,::=\,A+$\\
$A\,::=\,a\,|\,E$\\
$E\,::=\,1\,|\,E\,+\,E\,|\,E\,-\,E\,|\,E\,*\,E\,|\,E\,/\,E\,|\,E\,>\,E\,|\,...$ 25 more like it ...\\
input = $a\,*\,50000\,|\,a\,*\,100000\,|\,a\,*\,150000\,|\,a\,*\,200000$

\begin{table}[H]
\centering
\begin{tabular}{ | p{5em} | p{7em} | p{6em} | }
  \hline
  Input chars & Average Time & Lowest Time \\
  \hline
  50000 & 423 & 400 \\
  100000 & 786 & 780 \\
  150000 & 1183 & 1170 \\
  200000 & 1600 & 1590 \\
  \hline
\end{tabular}
\caption{Not left factored, not prefix shared; parse times}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | p{5em} | p{7em} | p{6em} | }
  \hline
  Input chars & Average Time & Lowest Time \\
  \hline
  50000 & 383 & 380 \\
  100000 & 726 & 720 \\
  150000 & 1090 & 1080 \\
  200000 & 1443 & 1440 \\
  \hline
\end{tabular}
\caption{Left factored; parse times}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | p{5em} | p{7em} | p{6em} | }
  \hline
  Input chars & Average Time & Lowest Time \\
  \hline
  50000 & 170 & 160 \\
  100000 & 326 & 320 \\
  150000 & 476 & 470 \\
  200000 & 630 & 630 \\
  \hline
\end{tabular}
\caption{Not left factored, prefix shared; parse times}
\end{table}

TODO: Add graph containing all of the above.

\end{document}
