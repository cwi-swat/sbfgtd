GSS:
-Every production consists of a number of nodes with a unique id. (For example S ::= AB | BA, A ::= a, B ::= b has the nodes S(-1), A(0), B(1), B(2), A(3), a(4) and b(5)).
-Every node in the GSS is unique per start position (N).
-Every node has one unique 'result' per substring (start + end position) (N^2).
-Every node has edges that point back to their 'parent(s)' (the node(s) that 'queued' / 'expected' it) (N).
-Every node has a reference to the 'next' node in the production.
-The recognizer is O(N^3) worst case (N nodes * N edges that are followed N times (once per substring); thus N * N * N).

-------------------------------------------------------------------------------------

Basic recognizer algorithm:
1. Expand the left most node that did not match anything yet on all stacks, until no stacks can be expanded anymore (i.e. they all have a terminal at the bottom).
2. Match all nodes on the bottom of the stacks to the input. Reduce the ones that match, remove the ones that don't.
3. Reduce the nodes on all stacks that matched something and have them queue the 'next' node in the production they are part of where applicable, until no reductions are possible anymore.
4. goto 1.

Recognizer pseudocode:
parse(){
	toExpandList.push('startNode');
	expand();
	
	do{
		reduce();
		expand();
	}while(notEmpty(todoList));
	
	if(notAtEOI()) parse error;
}

expand(){
	while(notEmpty(toExpandList)){
		node = toExpandList.pop();
		
		Node[] children = getChildrenFor(node);
		for(childNode <- children){
			if(reducedList.contains(childNode)){ // Child already has results from difference path
				if(reducedList.notContains(node)){ // Sharing
					todoList.add(node);
				}
			}else{
				if(expandedList.contains(childNode)){ // Sharing
					expandedList.get(childNode).addEdge(node);
				}else{
					expandedList.add(childNode);
				
					if(isTerminal(childNode)){
						todoList.push(childNode);
					}else{
						toExpandList.push(childNode);
					}
				}
			}
		}
	}
}

reduce(){
	while(!todoList.isEmpty()){
		node = todoList.pop();
		reducedList.add(node);
		
		if(node.hasEdges()){
			for(parent <- node.edges){
				if(todoList.notContains(parent)){ // Sharing
					todoList.push(parent);
				}
			}
		}else if(node.hasNext()){
			next = node.next;
			if(toExpand.contains(next)){ // Sharing
				next = toExpandList.get(next);
			}else{
				toExpandList.push(next);
			}
			next.addEdges(node.edges);
		}
	}
}

Example trace:
S ::= AB
A ::= a
B ::= b
input = ab

1. expand S
2. expect AB
3. expand A
4. expect a
5. reduce a and follow edge to A
6. move from A to B
7. expand B
8. expect b
9. reduce b follow edge to B
10. reduce AB and follow edge to S
11. reduce S
12. done.


Example trace 2 (left recursive):
S ::= A
A ::= Aa | a
input = aaa

1. expand S
2. expect A
3. expand A
4. expect Aa | a
5. expand A -> A shared
6. reduce a and follow edge to A
7. move from A to a
8. reduce Aa and follow edges to S and A
9. parse for S is incomplete and is discarded
10. move from A to a
11. reduce Aa and follow edges to S and A
12. parse for S is complete.
13. move from A to a
14. reduce Aa failed, since EOI has already been reached.
15. done.

-------------------------------------------------------------------------------------

Parse forest:
-'Deflattenized' / binarized.
-Every result node + prefixesList pair is unique.
-Every prefixesList has a unique start and end position.
-Every prefix in the prefixesList represents a different parse of the substring the prefixesList denotes.
-Every result node represents a parse for a certain substring (start + end position).
-All prefixesLists used in combination with the same 'type' of result node are the same, regardless of the result's end position.
-Every pair is identified by it's prefixesList's start position, result node start position and result node end position; the prefix end position and result node start position are the same (naturally).
-The parse forest contains O(N^2) result node's and O(N^2) prefixesLists. Since all prefixesLists are shared regardless of the result node's end position that they are matched to, there are at most O(N^2) unique links from result nodes to prefixesLists; making the parse forest O(N^2) worst case.

TODO: give worst case example.

Worst case grammar for worst case parse tree:
S ::= A | AA | AAA | AAAA | and so on ....
A ::= epsilon | a | aa | aaa | aaaa | and so on ....

input length	| Number of nodes	| Number of links	| Increase in nodes / links compared to previous level
0		| 1			| 1			| -
1		| 3			| 3			| 2
2		| 6			| 6			| 3
3		| 10			| 10			| 4
4		| 15			| 15			| 5

-------------------------------------------------------------------------------------

Parse tree example:
Grammar:
S ::= AAAA | AAA | AA
A ::= a | aa
input = aaaa

Visual representation of the parse forest, showing all alternatives for 'S0-4' (numbers indicate start and end position of the matched substring):
A0-1 <- A1-2 <- A2-3 <- A3-4
 \- A1-3 <--\-----/----/
A0-2 <-------\---/
 \            \-------- A2-4
  \---------------------/

Derivations (flattened version of the parse forest above):
S(A(a),A(a),A(a),A(a))	A0-1 <- A1-2 <- A2-3 <- A3-4
S(A(a),A(aa),A(a))	A0-1 <- A1-3 <- A3-4
S(A(aa),A(a),A(a))	A0-2 <- A2-3 <- A3-4
S(A(a),A(a),A(aa))	A0-1 <- A1-2 <- A2-4
S(A(aa),A(aa))		A0-2 <- A2-4

-------------------------------------------------------------------------------------

Parser psuedocode:
parse(){
	toExpandList.push('startNode');
	expand();
	
	do{
		reduce();
		expand();
	}while(notEmpty(todoList));
	
	if(notAtEOI()) parse error;
	
	return findResultStore('startNode');
}

expand(){
	while(notEmpty(toExpandList)){
		node = toExpandList.pop();
		
		Node[] children = getChildrenFor(node);
		for(childNode <- children){
			if(reducedList.contains(childNode)){
				if(reducedList.notContains(node)){
					todoList.add(node);
				}
			}else{
				if(expandedList.contains(childNode)){ // Sharing
					expandedList.get(childNode).addEdge(node);
				}else{
					expandedList.add(childNode);
				
					if(isTerminal(childNode)){
						todoList.push(childNode);
					}else{
						toExpandList.push(childNode);
					}
				}
			}
		}
	}
}

reduce(){
	while(!todoList.isEmpty()){
		node = todoList.pop();
		
		if(node.hasEdges()){
			for(parent <- node.edges){
				findResultStore(parent).addAlternative(node.prefixes, node.results);
				if(todoList.notContains(parent)){ // Sharing
					todoList.push(parent);
				}
			}
		}else if(node.hasNext()){
			next = node.next;
			if(toExpand.contains(next)){ // Sharing
				next = toExpandList.get(next);
			}else{
				toExpandList.push(next);
			}
			next.addEdges(node.edges);
			next.constructPrefixes(node.prefixes, node.results);
		}
	}
}

findResultStore(node){
	return resultStores.findOrCreate(node.startLocation, node.endLocation, node.name);
}

NOTE: algorithm / psuedocode was reverse engineered from the actual implementation and has been made more generic. (I'm currently unsure whether or not I accidentially ommitted any special cases in the conversion).

-------------------------------------------------------------------------------------

Optimizations:
-Make the algortihm breadth first to make it level synchronized; this reduces the amount of resources needed to keep track of sharing and reduces maximum memory usage in the general case.
-Match on entire literals instead of characters. This reduces stack activity and improves performance.
-Cache 'queued' / 'expected' children for every type of node per level, to ensure linear scaling for non-left-factored grammars and to enable further optimization.
-Don't store results and prefixesLists on edges; this reduces memory usage, since edges can remain pointers. Additionally edges can be shared between different nodes, since often nodes are guaranteed to (either partially or entirely) have the same edges, since they are always 'queued' / 'expected' by the same 'parent(s)'. The number of edges can be reduced to N * number of productions instead of N^2 * number of productions. In absence of a priority system that restricts 'parent' / 'child' relations, the number of edges can even be reduced to N * number of LHS's instead of N * number of productions (with as positive side effect that the parser for any grammar becomes as fast as it's left-factored counterpart (if implemented properly)).
-Share results between nodes with the same 'name', representing the same substring, but with a different id. (For example in "S ::= AB | CB, A ::= a, B ::= b, C ::= a" both B's can share results). This increases sharing in the resulting tree and improves scaling.
-The sharing check on 'parents' when reducing can be eliminated. Since every parent of the same 'type' always has exactly the same children for the same position, it is sufficient to just follow the edges of the first child and only add the results to the 'result store' of the parent(s) for all following children. This reduces scaling of the parser with respect to the number of different 'sorts' in the grammar from O(N^2) to O(N). (This optimization requires the previous optimization).
-Share 'prefixes' of productions in the GSS. (For example S ::= E + E | E - E both start with an E that matches the same substring(s), thus may as well be merged (into S ::= E (+ E | - E))).

-------------------------------------------------------------------------------------

Extended features:
-Lists are special-case GSS nodes that contain logic for stack expansion. (For example S ::= A* will queue A with a 'next' pointer to itself and 'epsilon'). So in a way, lists are implemented as dynamicly growing productions.
-Separated lists are similar to non-separated lists, with separator nodes added in between (For example S ::= {A B}* results in queueing the 'cyclic production': A -> B -> A and 'epsilon').
-Optionals are similar to lists, but only expect one 'child' + 'epsilon'.
-Start-of-line, end-of-line and at-column stack nodes are also available. These don't consume any input, but work as a kind of restriction. In the resulting tree they are added as a 'special' kind of epsilon.

-------------------------------------------------------------------------------------

Filtering:
-Look-ahead: Prevents 'expects' from 'firing'. Currently one character, but has no limit.
-Follow restrictions: If the current node being reduced is followed by any of its associated follow restrictions, the reduction is not performed. Follow restrictions can be any IMatchable node (character / literal / ciliteral / start-of-line / end-of-line / at-column).
-Rejects: Any RHS can be marked as reject. If it 'returns a result', any alternatives for the corresponding LHS representing the same substring are then discarded.
-Priorities and associativity: Are implemented as 'don't nest' relations. (For example E ::= E * E > E + E means that E + E can't be a child of E * E, similarly S ::= E + E {left} means that E + E can't be a child of the E of the left side of the production).

-------------------------------------------------------------------------------------

Benchmarks:

Worst case:
S ::= SSS | SS | a

Recognizer:
------------------------------------------
Input chars	| Average Time	| Lowest Time
------------------------------------------
50		| 4 ms		| 0 ms
100		| 20 ms		| 20 ms
150		| 70 ms		| 70 ms
200		| 190 ms	| 190 ms
250		| 416 ms	| 410 ms
300		| 800 ms	| 800 ms
350		| 1404 ms	| 1400 ms
400		| 2284 ms	| 2280 ms
450		| 3524 ms	| 3520 ms
500		| 5200 ms	| 5200 ms

Parser:
----------------------------------------------
Input chars	| Average Time	| Lowest Time
----------------------------------------------
50		| 6 ms		| 0 ms
100		| 36 ms		| 30 ms
150		| 130 ms	| 130 ms
200		| 332 ms	| 320 ms
250		| 672 ms	| 660 ms
300		| 1206 ms	| 1200 ms
350		| 2018 ms	| 2010 ms
400		| 3184 ms	| 3170 ms
450		| 4776 ms	| 4750 ms
500		| 6880 ms	| 6870 ms

Grammar factoring & prefix sharing:
This test tries to emulate a 'worst case' 'realistic' grammar in terms of size and type of productions. I.e. lots of left recursive stuff, but no ambiguous input.

Grammar:
S ::= A+
E ::= E + E | E - E | E * E | E / E | E > E | ... 25 more like it ... | 1
A ::= E | a
input = a * 50000 | a * 100000 | a * 150000 | a * 200000

Not left factored, not prefix shared; parse times:
----------------------------------------------
Input chars	| Average Time	| Lowest Time
----------------------------------------------
50000		| 423 ms	| 400 ms
100000		| 786 ms	| 780 ms
150000		| 1183 ms	| 1170 ms
200000		| 1600 ms	| 1590 ms

Left factored; parse times:
----------------------------------------------
Input chars	| Average Time	| Lowest Time
----------------------------------------------
50000		| 383 ms	| 380 ms
100000		| 726 ms	| 720 ms
150000		| 1090 ms	| 1080 ms
200000		| 1443 ms	| 1440 ms

Not left factored, prefix shared; parse times:
----------------------------------------------
Input chars	| Average Time	| Lowest Time
----------------------------------------------
50000		| 170 ms	| 160 ms
100000		| 326 ms	| 320 ms
150000		| 476 ms	| 470 ms
200000		| 630 ms	| 630 ms

