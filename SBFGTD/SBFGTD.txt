SBFGTD: 
-Scannerless Binary Forest Generalized Top Down parser, or Scannerless Bloody Fast Generalized Top Down parser, whatevery suits you best; it's both true ;-). I tried to make the name more ambiguous and still make sense, but came up with nothing so far ....

-------------------------------------------------------------------------------------

GSS:
-Every production consists of a number of nodes with a unique id. (For example S ::= AB | BA, A ::= a, B ::= b has the nodes S(-1), A(0), B(1), B(2), A(3), a(4) and b(5)). Every node must have a unique id, regardless whether or not the LHS's they are used in are completely or partially comparable.
-Every node in the GSS is unique per start position (N).
-Every node has one unique 'result' per substring (start + end position) (N^2).
-Every node has edges that point back to their 'parent(s)' (the node(s) that 'queued' / 'expected' it) (N).
-Every node has a reference to the 'next' node in the production.
-The recognizer is O(N^3) worst case (N nodes * N edges that are followed N times (once per substring); thus N * (N * N)).

-------------------------------------------------------------------------------------

Basic recognizer algorithm:
1. Expand the left most node of every production that did not match anything yet on all stacks, until no stacks can be expanded anymore (i.e. they all have a terminal at the bottom).
2. Match all nodes on the bottom of the stacks to the input. Reduce the ones that match, remove the ones that don't.
3. Reduce the nodes on all stacks that matched something and have them queue the 'next' node in the production they are part of where applicable, until no reductions are possible anymore.
4. goto 1.

Recognizer pseudocode:
parse(){
	toExpandList.push('startNode');
	expand();
	
	do{
		reduce();
		expand();
	}while(toReduceList.notEmpty());
	
	if(notAtEOI()) parse error;
}

expand(){
	while(toExpandList.notEmpty()){
		node = toExpandList.pop();
		
		Node[] children = getChildrenFor(node);
		for(childNode <- children){
			if(reducedStore.contains(childNode)){ // Child already has results from different path
				if(notAddedEarlierInThisLoop){ // Sharing
					toReduceList.add(node);
				}
			}else{
				if(expandedStore.contains(childNode)){ // Sharing
					expandedStore.get(childNode).addEdge(node);
				}else{
					expandedStore.add(childNode);
				
					if(childNode.isTerminal()){
						toReduceList.push(childNode);
					}else{
						toExpandList.push(childNode);
					}
				}
			}
		}
	}
}

reduce(){
	while(toReduceList.notEmpty()){
		node = toReduceList.pop();
		reducedStore.add(node);
		
		if(node.hasEdges()){
			for(parent <- node.edges){
				if(toReduceList.notContains(parent)){ // Sharing
					toReduceList.push(parent);
				}
			}
		}else if(node.hasNext()){
			next = node.next;
			if(toExpand.contains(next)){ // Sharing
				next = toExpandList.get(next);
			}else if(expandedStore.notContains(next)){ // Sharing
				toExpandList.push(next);
			}
			next.addEdges(node.edges);
		}
	}
}

Example trace:
S ::= AB
A ::= a
B ::= b
input = ab

1. expand S
2. expect AB
3. expand A
4. expect a
5. reduce a and follow edge to A
6. move from A to B
7. expand B
8. expect b
9. reduce b follow edge to B
10. reduce AB and follow edge to S
11. reduce S
12. done.


Example trace 2 (left recursive):
S ::= A
A ::= Aa | a
input = aaa

1. expand S
2. expect A
3. expand A
4. expect Aa | a
5. expand A -> A shared
6. reduce a and follow edge to A
7. move from A to a
8. reduce Aa and follow edges to S and A
9. parse for S is incomplete and is discarded
10. move from A to a
11. reduce Aa and follow edges to S and A
12. parse for S is complete.
13. move from A to a
14. reduce Aa failed, since EOI has already been reached.
15. done.

TODO: Maybe add an illustration?

-------------------------------------------------------------------------------------

Parse forest:
-Custom 'deflattenized' format (similar in intend as binarization).
-Consists of result nodes, prefixesLists and pairs of result nodes and prefixesLists.
-Every result node + prefixesList pair is unique.
-Every prefixesList represent all parses for a certain substring (start + end position).
-Every prefix in the prefixesList represents a different parse of the substring the prefixesList denotes (start + end position).
-Every result node represents a parse for a certain substring (start + end position).
-All prefixesLists used in combination with the same 'type' of result node are the same, regardless of the result's end position.
-Every pair is identified by it's prefixesList's start position, result node start position and result node end position; the prefix end position and result node start position are the same (naturally).
-The parse forest contains at most O(N^2) result node's and O(N^2) prefixesLists. Since all prefixesLists are shared regardless of the result node's end position that they are matched to, there are at most O(N^2) unique links from result nodes to prefixesLists; making the parse forest O(N^2) worst case (and me a genious) (see example further below).
-Additionally it is possible to output a flattened (unbound polynomial) version of the parse forest at the user's request.

-------------------------------------------------------------------------------------

Parse tree example:
Grammar:
S ::= AAAA | AAA | AA
A ::= a | aa
input = aaaa

(Partial) visual representation of the parse forest, showing all alternatives for 'S0-4' (numbers indicate start and end position of the matched substring):
A0-1 <- A1-2 <- A2-3 <- A3-4
 \- A1-3 <--\-----/----/
A0-2 <-------\---/
 \            \-------- A2-4
  \---------------------/

(Complete) flattened version of the parse forest above (showing all derivations):
S(A(a),A(a),A(a),A(a))	A0-1 <- A1-2 <- A2-3 <- A3-4
S(A(a),A(aa),A(a))	A0-1 <- A1-3 <- A3-4
S(A(aa),A(a),A(a))	A0-2 <- A2-3 <- A3-4
S(A(a),A(a),A(aa))	A0-1 <- A1-2 <- A2-4
S(A(aa),A(aa))		A0-2 <- A2-4

TODO: Chang into illustration of a parse forest and add more text.

-------------------------------------------------------------------------------------

Worst case parse tree example (to prove O(N^2) size):

Grammar:
S ::= SSS | SS | a | epsilon
input = a * 1 - a * 10

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Input length	| Number of nodes	| Number of pairs / links	| Number of parent -> child pointers	| Total		| Increment compared to previous level	| Total / N^2
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
0		| 2			| 2				| 3					| 7		| -					| -
1		| 5			| 6				| 9					| 20		| 13					| 20
2		| 8			| 12				| 15					| 35		| 15					| 8.75
3		| 11			| 20				| 21					| 52		| 17					| 5.777777778
4		| 14			| 30				| 27					| 71		| 19					| 4.4375
5		| 17			| 42				| 33					| 92		| 21					| 3.68
6		| 20			| 56				| 39					| 115		| 23					| 3.194444444
7		| 23			| 72				| 45					| 140		| 25					| 2.857142857
8		| 26			| 90				| 51					| 167		| 27					| 2.609375
9		| 29			| 110				| 57					| 196		| 29					| 2.419753086
10		| 32			| 132				| 63					| 227		| 31					| 2.27

This example clearly shows better then O(N^2) scaling ((Total / N^2) gets lower as N increases). Adding more 'items' to the grammar may increase the number of nodes / links / pointers by a factor (depending on what you add), but it will have no effect on the worst-case space bound.

NOTE: GSS prefix sharing was enabled in this example; when disabled the number of pairs / links increases by 50%, since then there are 3, instead of 2 'nodes' that require prefixes.

TODO: Improve explanation.

-------------------------------------------------------------------------------------

Parser psuedocode:
parse(){
	toExpandList.push('startNode');
	expand();
	
	do{
		reduce();
		expand();
	}while(toReduceList.notEmpty());
	
	if(notAtEOI()) parse error;
	
	return findResultStore('startNode');
}

expand(){
	while(toExpandList.notEmpty()){
		node = toExpandList.pop();
		
		Node[] children = getChildrenFor(node);
		for(childNode <- children){
			if(reducedStore.contains(childNode)){
				findResultStore(node).addAlternative(childNode.prefixes, childNode.results);
				if(notAddedEarlierInThisLoop){ // Sharing
					toReduceList.add(node);
				}
			}else{
				if(expandedStore.contains(childNode)){ // Sharing
					expandedStore.get(childNode).addEdge(node);
				}else{
					expandedStore.add(childNode);
				
					if(childNode.isTerminal()){
						toReduceList.push(childNode);
					}else{
						toExpandList.push(childNode);
					}
				}
			}
		}
	}
}

reduce(){
	while(toReduceList.notEmpty()){
		node = toReduceList.pop();
		reducedStore.add(node);
		
		if(node.hasEdges()){
			for(parent <- node.edges){
				findResultStore(parent).addAlternative(node.prefixes, node.results);
				if(toReduceList.notContains(parent)){ // Sharing
					if(reducedStore.notContains(parent)){
						toReduceList.push(parent);
					}
				}
			}
		}else if(node.hasNext()){
			next = node.next;
			if(toExpandList.contains(next)){ // Sharing
				next = toExpandList.get(next);
			}else if(expandedStore.notContains(next)){ // Sharing
				toExpandList.push(next);
			}
			next.addEdges(node.edges);
			next.updatePrefixes(node.prefixes, node.results);
		}
	}
}

findResultStore(node){
	return resultStores.findOrCreate(node.startLocation, node.endLocation, node.name);
}

NOTE: algorithm / psuedocode was reverse engineered from the actual implementation and has been made more generic. (I'm currently unsure whether or not I accidentially ommitted any special cases in the conversion).

-------------------------------------------------------------------------------------

Optimizations:
-Make the algortihm breadth first / level synchronized; this reduces the amount of resources needed to keep track of sharing and reduces maximum memory usage in the general case.
-Match on entire literals instead of characters. This reduces stack activity and improves performance.
-Cache 'queued' / 'expected' children for every type of node per level, to ensure linear scaling for non-left-factored grammars and to enable further optimization.
-Don't store results on edges, but externally in a (temporary / reusable) per level store; this reduces memory usage, since edges can remain pointers. Additionally edges can be shared between different nodes, since often nodes are guaranteed to (either partially or entirely) have the same edges, since they are always 'queued' / 'expected' by the same 'parent(s)'. The number of edges can be reduced to N * number of productions instead of N * (number of productions ^ 2). In absence of a priority system that restricts 'parent' / 'child' relations, the number of edges can even be reduced to N * number of LHS's instead of N * number of productions (with as positive side effect that the parser for any grammar becomes as fast as it's left-factored counterpart (if implemented properly)).
-Share results between nodes with the same 'name', representing the same substring, but with a different id. (For example in "S ::= AB | CB, A ::= a, B ::= b, C ::= a" both B's can share results). This increases sharing in the resulting tree and improves scaling.
-The sharing check on 'parents' when reducing can be eliminated. Since every parent of the same 'type' always has exactly the same children for the same position, it is sufficient to just follow the edges of the first child and only add the results to the 'result store' of the parent(s) for all following children. This changes the scaling of the parser with respect to the number of different 'sorts' in the grammar from quadratic to linear. (This optimization requires the previous optimization).
-Share 'prefixes' of productions in the GSS. (For example S ::= E + E | E - E both start with an E that matches the same substring(s), thus may as well be merged (into S ::= E (+ E | - E))); this is done by simply giving both E's the same id and allowing every node to have more then one 'next' node.

TODO: Add more explanation + examples for all of these.

-------------------------------------------------------------------------------------

Extended features:
-Lists are special-case GSS nodes that contain logic for stack expansion. (For example S ::= A* will queue A with a 'next' pointer to itself and 'epsilon'). So in a way, lists are implemented as dynamicly growing productions.
-Separated lists are similar to non-separated lists, with separator nodes added in between (For example S ::= {A B}* results in queueing the 'cyclic production': A -> B -> A and 'epsilon').
-Optionals are similar to lists, but only expect one 'child' + 'epsilon'.
-Start-of-line, end-of-line and at-column stack nodes are also available. These don't consume any input, but work as a kind of restriction. In the resulting tree they are added as a 'special' kind of epsilon.

TODO: Add examples / maybe some illustrations.

-------------------------------------------------------------------------------------

Filtering:
-Look-ahead: Prevents 'expects' from 'firing'. Currently one character, but has no limit.
-Follow restrictions: If the current node being reduced is followed by any of its associated follow restrictions, the reduction is not performed. Follow restrictions can be any IMatchable node (character / literal / ciliteral / start-of-line / end-of-line / at-column).
-Rejects: Any RHS can be marked as reject. If it 'returns a result', any alternatives for the corresponding LHS representing the same substring are then discarded.
-Priorities and associativity: Are implemented as 'don't nest' relations. (For example E ::= E * E > E + E means that E + E can't be a child of E * E, similarly S ::= E + E {left} means that E + E can't be a child of the E on the left side of the production).

-------------------------------------------------------------------------------------

Benchmarks (of the fully optimized version):

Worst case:
S ::= SSS | SS | a
input = a * 50 - a * 500

Recognizer:
------------------------------------------
Input chars	| Average Time	| Lowest Time
------------------------------------------
50		| 4 ms		| 0 ms
100		| 20 ms		| 20 ms
150		| 70 ms		| 70 ms
200		| 190 ms	| 190 ms
250		| 416 ms	| 410 ms
300		| 800 ms	| 800 ms
350		| 1404 ms	| 1400 ms
400		| 2284 ms	| 2280 ms
450		| 3524 ms	| 3520 ms
500		| 5200 ms	| 5200 ms

Parser:
----------------------------------------------
Input chars	| Average Time	| Lowest Time
----------------------------------------------
50		| 6 ms		| 0 ms
100		| 36 ms		| 30 ms
150		| 130 ms	| 130 ms
200		| 332 ms	| 320 ms
250		| 672 ms	| 660 ms
300		| 1206 ms	| 1200 ms
350		| 2018 ms	| 2010 ms
400		| 3184 ms	| 3170 ms
450		| 4776 ms	| 4750 ms
500		| 6880 ms	| 6870 ms

TODO: Add graph.

-------------------------------------------------------------------------------------

Grammar factoring & prefix sharing:
This test tries to emulate a 'realistic' 'worst case' grammar in terms of size and type of productions. I.e. lots of left recursive stuff, but no ambiguous input.

Grammar:
S ::= A+
A ::= a | E
E ::= 1 | E + E | E - E | E * E | E / E | E > E | ... 25 more like it ...
input = a * 50000 | a * 100000 | a * 150000 | a * 200000

Not left factored, not prefix shared; parse times:
----------------------------------------------
Input chars	| Average Time	| Lowest Time
----------------------------------------------
50000		| 423 ms	| 400 ms
100000		| 786 ms	| 780 ms
150000		| 1183 ms	| 1170 ms
200000		| 1600 ms	| 1590 ms

Left factored; parse times:
----------------------------------------------
Input chars	| Average Time	| Lowest Time
----------------------------------------------
50000		| 383 ms	| 380 ms
100000		| 726 ms	| 720 ms
150000		| 1090 ms	| 1080 ms
200000		| 1443 ms	| 1440 ms

Not left factored, prefix shared; parse times:
----------------------------------------------
Input chars	| Average Time	| Lowest Time
----------------------------------------------
50000		| 170 ms	| 160 ms
100000		| 326 ms	| 320 ms
150000		| 476 ms	| 470 ms
200000		| 630 ms	| 630 ms

TODO: Add graph containing all of the above.

