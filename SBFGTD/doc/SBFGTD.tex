\documentclass[a4paper,10pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage{array}
\usepackage{graphicx}
\usepackage{float}

%opening
\title{Generalized Top Down Parsing In Cubic Time}
\author{Arnold Lankamp}

\begin{document}

\maketitle

\begin{abstract}

In this article we will describe a generalized top down parsing algorithm. Since it's top down it will be easier for a human to understand. Additionally we will also demonstrate that, in terms of efficiency, our implementation performes excellent.

\end{abstract}

\section{Introduction}

If the abstract didn't catch your attention yet, here are some highlights:
\begin{itemize}
 \setlength{\itemsep}{0pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}

 \item Worst case cubic space and time bounds with respect to the length of the input.
 \item Worst case linear scaling relative to the size of the grammar.
 \item Linear performance on LL(k) and LR(k) grammars.
 \item Generating or hand crafting the parser code is trivial.
 \item Parse traces are easy to follow; using a debugger to step through the parse process can actually be informative.
 \item Low constant overhead.
 \item No performance hit for rules that are not left factored.
\end{itemize}
We call our parser, the Scannerless Binary Forest Generalized Top Down Parser or SBFGTD (BF also means Bloody Fast, in case your wondering why it was put in the abbreviation).

\section{Recognizer}

\subsection{GSS}

As is common in generalized parsing, we use a kind of Graph Structured Stack (GSS). In our case we generate a stack node with a unique identifier for every item in the grammar. So for for example; if we would have the following grammar: $S\,::=\,AB\,|\,BA,\,A\,::=\,a,\,B\,::=\,b$, we would assign identifiers as follows: $S$(-1), $A$(0), $B$(1), $B$(2), $A$(3), $a$(4) and $b$(5). Every terminal and non-terminal on the right hand side of a production gets a unique identifier, regardless whether or not a comparable right hand side occurs again in another position in the grammar. These identifiers are needed to handle sharing in the GSS correctly. Each stack node consists of this identifier, edges back to their 'parents' and information about it's 'right neighbour' in the production, so we know where to go next.

Since the GSS only contains nodes that we expect to 'encounter', there are at most $O(N)$ of them; one of every type per location in the input string. Since a node only has one edge to each of it's possible parents per level and there are $O(N)$ levels, there are at most $O(N)$ edges per node. This means that the total algorithm becomes $O(N^3)$; in the worst case we follow $O(N^2)$ edges in total ... TODO: figure out where the other $O(N)$ is.

\subsection{Basic algorithm}

\begin{enumerate}
 \setlength{\itemsep}{0pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}

 \item Expand the left most node of every production that did not match anything yet on all stacks, until no stacks can be expanded anymore (i.e. they all have a terminal at the bottom).
 \item Match all nodes on the bottom of the stacks to the input. Reduce the ones that match, remove the ones that don't.
 \item Reduce the nodes on all stacks that matched something and have them queue the 'next' node in the production they are part of where applicable, until no reductions are possible anymore.
 \item goto 1.
\end{enumerate}

\subsection{Pseudocode}

{\small
\begin{verbatim}
parse(){
  toExpandSet.push('startNode');
  expand();
  
  do{
    reduce();
    expand();
  }while(toReduceSet.notEmpty());
  
  if(notAtEOI()) parse error;
}

expand(){
  while(toExpandSet.notEmpty()){
    node = toExpandSet.pop();
    
    Node[] children = getChildrenFor(node);
    for(childNode <- children){
      if(reducedStore.contains(childNode)){ // Child already has results
         if(notAddedEarlierInThisLoop){ // Sharing
           toReduceSet.add(node);
         }
      }else{
        if(expandedStore.contains(childNode)){ // Sharing
          expandedStore.get(childNode).addEdge(node);
        }else{
          expandedStore.add(childNode);
          
          if(childNode.isTerminal()){
            toReduceSet.push(childNode);
          }else{
            toExpandSet.push(childNode);
          }
        }
      }
    }
  }
}

reduce(){
  while(toReduceSet.notEmpty()){
    node = toReduceSet.pop();
    reducedStore.add(node);
    
    if(node.hasEdges()){
      for(parent <- node.edges){
        if(toReduceSet.notContains(parent) &&
            reducedStore.notContains(parent)){ // Sharing (is reduced)
          toReduceSet.push(parent);
        }
      }
    }else if(node.hasNext()){
      next = node.next;
      if(toExpand.contains(next)){ // Sharing
        next = toExpandSet.get(next);
      }else if(expandedStore.notContains(next)){ // Sharing
        toExpandSet.push(next);
      }
      next.addEdges(node.edges);
    }
  }
}
\end{verbatim}
}

\subsection{Example traces}

To illustrate how the recognizer works, here are some example traces.

\subsubsection{Strait forward}
$S\,::=\,AB$\\
$A\,::=\,a$\\
$B\,::=\,b$\\
input = $ab$

\begin{enumerate}
 \setlength{\itemsep}{0pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}
 
 \item expand S
 \item expect AB
 \item expand A
 \item expect a
 \item reduce a and follow edge to A
 \item move from A to B
 \item expand B
 \item expect b
 \item reduce b follow edge to B
 \item reduce AB and follow edge to S
 \item reduce S
 \item done.
\end{enumerate}

\subsubsection{Left recursive}
$S\,::=\,A$\\
$A\,::=\,Aa\,|\,a$\\
input = $aaa$

\begin{enumerate}
 \setlength{\itemsep}{0pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}
 
 \item expand S
 \item expect A
 \item expand A
 \item expect Aa $|$ a
 \item expand A $\Rightarrow$ A shared
 \item reduce a and follow edge to A
 \item move from A to a
 \item reduce Aa and follow edges to S and A
 \item parse for S is incomplete and is discarded
 \item move from A to a
 \item reduce Aa and follow edges to S and A
 \item parse for S is complete.
 \item move from A to a
 \item reduce Aa failed, since EOI has already been reached.
 \item done.
\end{enumerate}

TODO: Maybe convert to illustrations?

\section{Parser}

\subsection{Parse forest}

To represent the parse forest we use a format that was especially designed for this parser, to ensure worst-case behaviour remains within cubic time and space bounds. We call it 'deflattenized', for lack of a better description; it's intent is similar to binarization, but it's implementation is somewhat different.

The parse forest consists of nodes. Every node in the forest contains a 'result', which represents a substring for a certain terminal or non-terminal. In case of non-terminals, these 'results' contain one or more references to alternative representations of the substring it denotes. Each node also contains a set of prefixes. A prefix only consists of a reference to a node. If you trace all possible paths through the prefixes of a node to the start, you'll get all alternatives for a certain production at a certain location.

Every node in the tree is identified by the substring it represents and it's prefixes; all the prefixes of a certain node denote the same substring, which end were the 'result' contained in the node starts. All prefix sets are shared regardless of the node's end position that they are matched to, so there are at most $O(N)$ of them. Every prefix set can contain up to $N$ different prefixes. Since there can be only one 'result' per substring, this means there are at most $O(N^{2})$ 'results'. Overall this makes the parse forest $O(N^{3})$ worst-case, since the number of unique nodes is limited to $O(N^{3})$. We'll look into this with more detail further on in this chapter.

Additionally it is possible to output a flattened version of the parse forest at the user's request. Naturally this parse forest will be unbound polynomial in size in the worst-case. Pratically this unlikely to happen; moreso since filtering can be done during parsing and flattening, making it improbable that many ambiguities remain in the final tree. We'll discuss filtering later on in this document.

\subsubsection{Example}

Parse tree example:
Grammar:
$S\,::=\,AAAA\,|\,AAA\,|\,AA$\\
$A\,::=\,a\,|\,aa$\\
input = $aaaa$

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{a_aa-forest.png}
\caption{A partial visual representation of the parse forest, showing all alternatives for '$S0$-$4$'. The numbers indicate start and end position of the matched substring.}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{ p{15em} p{15em} }
Derivation & Forest path\\
\hline
S(A(a),A(a),A(a),A(a)) & $A0$-$1$ $\leftarrow$ $A1$-$2$ $\leftarrow$ $A2$-$3$ $\leftarrow$ $A3$-$4$\\
S(A(a),A(aa),A(a)) & $A0$-$1$ $\leftarrow$ $A1$-$3$ $\leftarrow$ $A3$-$4$\\
S(A(aa),A(a),A(a)) & $A0$-$2$ $\leftarrow$ $A2$-$3$ $\leftarrow$ $A3$-$4$\\
S(A(a),A(a),A(aa)) & $A0$-$1$ $\leftarrow$ $A1$-$2$ $\leftarrow$ $A2$-$4$\\
S(A(aa),A(aa)) & $A0$-$2$ $\leftarrow$ $A2$-$4$
\end{tabular}
\caption{'Flattened' version of the parse forest}
\end{table}

\subsubsection{Worst case example}

Worst-case parse tree example:

Grammar:\\
$S\,::=\,SSS\,|\,SS\,|\,a\,|\,\epsilon$\\
input = $a\,*\,1\,-\,a\,*\,10$

\begin{table}[H]
\centering
\begin{tabular}{ | p{6em} | p{4em} | p{4em} | p{4em} | p{3em} | p{5em} | }
  \hline
  Input length & Results & Nodes & Pointers & Total & Total / $N^{3}$ \\
  \hline
  0 & 2 & 2 & 3 & 7 & - \\
  1 & 5 & 6 & 9 & 20 & 20 \\
  2 & 8 & 13 & 15 & 36 & 4.5 \\
  3 & 11 & 24 & 21 & 56 & 2.074 \\
  4 & 14 & 40 & 27 & 81 & 1.265 \\
  5 & 17 & 62 & 33 & 112 & 0.896 \\
  6 & 20 & 91 & 39 & 150 & 0.694 \\
  7 & 23 & 128 & 45 & 196 & 0.571 \\
  8 & 26 & 174 & 51 & 251 & 0.490 \\
  9 & 29 & 230 & 57 & 316 & 0.433 \\
  10 & 32 & 297 & 63 & 392 & 0.392 \\
  \hline
\end{tabular}
\caption{Worst-case parse tree data}
\end{table}

\subsection{Psuedocode}

Augmenting our regonizer with parse tree construction code it trivial. Only a few minor adjustments are needed.

{\small
\begin{verbatim}

Parser psuedocode:
parse(){
  toExpandSet.push('startNode');
  expand();
  
  do{
    reduce();
    expand();
  }while(toReduceSet.notEmpty());
  
  if(notAtEOI()) parse error;
  
  return findResultStore('startNode');
}

expand(){
  while(toExpandSet.notEmpty()){
    node = toExpandSet.pop();
    
    Node[] children = getChildrenFor(node);
    for(childNode <- children){
      if(reducedStore.contains(childNode)){
        findResultStore(node).
            addAlternative(childNode.prefixes, childNode.results);
        if(notAddedEarlierInThisLoop){ // Sharing
          toReduceSet.add(node);
        }
      }else{
        if(expandedStore.contains(childNode)){ // Sharing
          expandedStore.get(childNode).addEdge(node);
        }else{
          expandedStore.add(childNode);
          
          if(childNode.isTerminal()){
            toReduceSet.push(childNode);
          }else{
            toExpandSet.push(childNode);
          }
        }
      }
    }
  }
}

reduce(){
  while(toReduceSet.notEmpty()){
    node = toReduceSet.pop();
    reducedStore.add(node);
    
    if(node.hasEdges()){
      for(parent <- node.edges){
        findResultStore(parent).
            addAlternative(node.prefixes, node.results);
        if(toReduceSet.notContains(parent) &&
            reducedStore.notContains(parent)){ // Sharing (is reduced)
          toReduceSet.push(parent);
        }
      }
    }else if(node.hasNext()){
      next = node.next;
      if(toExpandSet.contains(next)){ // Sharing
        next = toExpandSet.get(next);
      }else if(expandedStore.notContains(next)){ // Sharing
        toExpandSet.push(next);
      }
      next.addEdges(node.edges);
      next.updatePrefixes(node.prefixes, node.results);
    }
  }
}

findResultStore(node){
  return resultStores.
    findOrCreate(node.startLocation, node.endLocation, node.name);
}

\end{verbatim}
}

NOTE: algorithm / psuedocode was reverse engineered from the actual implementation and has been made more generic. (I'm currently unsure whether or not I accidentially ommitted any special cases in the conversion).

\section{Optimizations}

\subsection{General}

The basic algorithm is fairly strait forward and relatively simple to implement. The naïve implementation will respect worst-case cubic time and space bounds, however adaptations can be made to improve general case efficiency.

Making the implementation breath first / level synchronized generally reduces resource usage. This is because it enables us to determine when certain data is no longer necessary and can be discarded. Additionally, since much information is stored on a per-level basis we can exploit the assumption that we are level synchronized to our advantage. For example, the memory used for stacks that died off can be reclaimed and reused and sharing and such can be handled slightly more efficiently.

Another minor performance improvement can be made by matching on entire literals at once, instead of on characters. This decreases the amount of necessary GSS nodes and thus reduces stack activity.

\subsection{Edge related}

Generally parsers store results on edges in the GSS; likely for convience reasons. If we would refrain from doing this and store parse results elsewhere (in a table with constant look-up time), edges could remain pointers. At first sight this may not be an advantage, however we would be able to share edges amoung GSS nodes and it opens up numereous other interesting opportunities for optimization.

First of all, by caching 'expected children' for every type of node per level a substantial performance increase can be achieved. Namely, this will ensure linear scaling for non-left-factored grammars. The reason for this is the following; for example, if we take the grammar rule: $E\,::=\,E\,+\,E\,|\,E\,-\,E$. The expansion at the first level would look like this:
\begin{enumerate}
 \setlength{\itemsep}{0pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}
 
 \item $E$(0) $\rightarrow$ $E$(1) $+$ $E$(2)
 \item $E$(1) edges = {$E$(0)}
 \item $E$(0) $\rightarrow$ $E$(3) $-$ $E$(4)
 \item $E$(3) edges = {$E$(0)}
 \item $E$(1) $\rightarrow$ $E$(1) $+$ $E$(2)
 \item $E$(1) edges = {$E$(0), $E$(1)}
 \item $E$(1) $\rightarrow$ $E$(3) $-$ $E$(4)
 \item $E$(3) edges = {$E$(0), $E$(1)}
 \item $E$(3) $\rightarrow$ $E$(1) $+$ $E$(2)
 \item $E$(1) edges = {$E$(0), $E$(1), $E$(3)}
 \item $E$(3) $\rightarrow$ $E$(3) $-$ $E$(4)
 \item $E$(3) edges = {$E$(0), $E$(1), $E$(3)}
\end{enumerate}
Clearly demonstrating quardratic behaviour. If we would remember what the edges set of one $E$, we can reuse it for any other $E$ in the same level and update it with additional edges by reference; this is possible since nodes are guaranteed to have the same edges if they are 'expected' by the same 'parent(s)'. This will make the expansion look like this:
\begin{enumerate}
 \setlength{\itemsep}{0pt}
 \setlength{\parskip}{0pt}
 \setlength{\parsep}{0pt}
 
 \item $E$(0) $\rightarrow$ $E$(1) $+$ $E$(2)
 \item $E$(0) $\rightarrow$ $E$(3) $-$ $E$(4)
 \item $E$ edges = {$E$(0)}
 \item $E$(1) $\Rightarrow$ $E$ edges += $E$(1)
 \item $E$(3) $\Rightarrow$ $E$ edges += $E$(3)
\end{enumerate}
Now expanding clearly completes in linear time. An additional benefit is that all the edge sets are shared between the children of the different $E$'s, saving memory. It will reduce the worst-case number of edges to $N\,*\,\mathit{numberOfLHS's}$. Originally this would have been $N\,*\,\mathit{numberOfProductions}^{2}$. The major benefit is, that this optimization lessens the need for left-factoring of grammars. In absence of look-ahead filtering performance should be on par; in other cases it may be close enough to remove it's necessity.

Finally, we can also gain something at the other end. The 'is reduced' check on nodes can be eliminated. Since every GSS node of the same 'sort' always has exactly the same children if they are in the same level, it is sufficient to initially just follow the first edge in an edge set. In case the node this edge points to has already been reduced in this level, nothing needs to be done (except record the alternative, in case we're parsing and not just recognizing); otherwise all other edges need to be followed as normally would have been the case. Since this guarantees a node can never be queued for reduction more then once, we can remove the check. This improves performance by a factor between one and the number of non-terminals in the grammar.

\subsection{GSS}

-Share 'prefixes' of productions in the GSS. (For example $S\,::=\,E\,+\,E\,|\,E\,-\,E$ both start with an E that matches the same substring(s), thus may as well be merged (into $S\,::=\,E\,(+\,E\,|\,-\,E)$)); this is done by simply giving both $E$'s the same id and allowing every node to have more then one 'next' node.

\section{Filtering}

\subsection{Look-ahead}

-Prevents 'expects' from 'firing'. Currently one character, but has no limit.

\subsection{Follow restrictions}

-If the current node being reduced is followed by any of its associated follow restrictions, the reduction is not performed. Follow restrictions can be any IMatchable node (character / literal / ciliteral / start-of-line / end-of-line / at-column).

\subsection{Rejects}

-Any RHS can be marked as reject. If it 'returns a result', any alternatives for the corresponding LHS representing the same substring are then discarded.\\
-Partially parse time, partially post parse filtering. \\
TODO: Check correctness.

\subsection{Priorities and associativity}

-Are implemented as 'don't nest' relations. (For example E ::= E * E > E + E means that E + E can't be a child of E * E, similarly S ::= E + E {left} means that E + E can't be a child of the E on the left side of the production).\\
-Completely parse time.\\
NOTE: Disables certain optimizations (hurts badly) and is a pain to implement.

TODO: add examples.

\subsection{Actions}

-Every sort can have a action associated with it.\\
-Can be used for disambiguation, amoung other things.

\section{Benchmarks}

\subsection{Worst case}

$S\,::=\,SSS\,|\,SS\,|\,a$\\
input = $a\,*\,50\,-\,a\,*\,500$

\begin{table}[H]
\centering
\begin{tabular}{ | p{5em} | p{7em} | p{6em} | }
  \hline
  Input chars & Average Time & Lowest Time \\
  \hline
  50 & 4 & 0 \\
  100 & 18 & 10 \\
  150 & 60 & 60 \\
  200 & 154 & 150 \\
  250 & 322 & 320 \\
  300 & 596 & 590 \\
  350 & 996 & 990 \\
  400 & 1534 & 1530 \\
  450 & 2228 & 2220 \\
  500 & 3090 & 3080 \\
  \hline
\end{tabular}
\caption{Recognizer}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | p{5em} | p{7em} | p{6em} | }
  \hline
  Input chars & Average Time & Lowest Time \\
  \hline
  50 & 6 & 0 \\
  100 & 36 & 30 \\
  150 & 130 & 130 \\
  200 & 332 & 320 \\
  250 & 672 & 660 \\
  300 & 1206 & 1200 \\
  350 & 2018 & 2010 \\
  400 & 3184 & 3170 \\
  450 & 4776 & 4750 \\
  500 & 6880 & 6870 \\
  \hline
\end{tabular}
\caption{Parser}
\end{table}

NOTE: Recognizer optimized for speed; parser for memory and speed (meaning it can be faster).

TODO: Add graph.

\subsection{Grammar factoring}

This test tries to emulate a 'realistic' 'worst-case' grammar in terms of size and type of productions. I.e. lots of left recursive stuff, but no ambiguous input.

Grammar:\\
$S\,::=\,A+$\\
$A\,::=\,a\,|\,E$\\
$E\,::=\,1\,|\,E\,+\,E\,|\,E\,-\,E\,|\,E\,*\,E\,|\,E\,/\,E\,|\,E\,>\,E\,|\,...$ 25 more like it ...\\
input = $a\,*\,50000\,|\,a\,*\,100000\,|\,a\,*\,150000\,|\,a\,*\,200000$

\begin{table}[H]
\centering
\begin{tabular}{ | p{5em} | p{7em} | p{6em} | }
  \hline
  Input chars & Average Time & Lowest Time \\
  \hline
  50000 & 423 & 400 \\
  100000 & 786 & 780 \\
  150000 & 1183 & 1170 \\
  200000 & 1600 & 1590 \\
  \hline
\end{tabular}
\caption{Not left factored, not prefix shared; parse times}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | p{5em} | p{7em} | p{6em} | }
  \hline
  Input chars & Average Time & Lowest Time \\
  \hline
  50000 & 383 & 380 \\
  100000 & 726 & 720 \\
  150000 & 1090 & 1080 \\
  200000 & 1443 & 1440 \\
  \hline
\end{tabular}
\caption{Left factored; parse times}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{ | p{5em} | p{7em} | p{6em} | }
  \hline
  Input chars & Average Time & Lowest Time \\
  \hline
  50000 & 170 & 160 \\
  100000 & 326 & 320 \\
  150000 & 476 & 470 \\
  200000 & 630 & 630 \\
  \hline
\end{tabular}
\caption{Not left factored, prefix shared; parse times}
\end{table}

TODO: Add graph containing all of the above.

\end{document}
